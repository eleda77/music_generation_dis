{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import music21\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_midi(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file is valid MIDI file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path: String path to the MIDI file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            header = f.read(4)\n",
    "            if header != b'MThd':\n",
    "                return False\n",
    "            \n",
    "        converter.parse(file_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def data_extractor(directory):\n",
    "    \"\"\"\n",
    "    Function converts midi files to metadata and appends nested metadata lists into one large list\n",
    "    composed of all the songs in the dataset.\n",
    "    \"\"\"\n",
    "    notes = []\n",
    "    offsets = []\n",
    "    durations = []\n",
    "    count = 0\n",
    "    total_files = 0\n",
    "    skipped_files = 0\n",
    "    \n",
    "    # Get all matching files\n",
    "    files = glob.glob(directory)\n",
    "    total_files = len(files)\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Verify the file is a valid MIDI file\n",
    "            if not is_valid_midi(file):\n",
    "                print(f\"Skipping invalid MIDI file: {file}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "                \n",
    "            mid = converter.parse(file)\n",
    "            notes_to_parse = None\n",
    "            prev_offset = 0\n",
    "            count += 1\n",
    "            print(f\"Processing FILE: {file}, no. {count}/{total_files}\")\n",
    "\n",
    "            try:\n",
    "                s2 = instrument.partitionByInstrument(mid)\n",
    "                if s2 and s2.parts:  # Check if parts exist\n",
    "                    notes_to_parse = s2.parts[0].recurse()\n",
    "                else:\n",
    "                    notes_to_parse = mid.flat.notes\n",
    "            except:\n",
    "                notes_to_parse = mid.flat.notes\n",
    "\n",
    "            if not notes_to_parse:  # Skip if no notes found\n",
    "                print(f\"No notes found in {file}, skipping...\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                    durations.append(str(element.quarterLength))\n",
    "                    offset_dif = float(element.offset-prev_offset)\n",
    "                    offsets.append(round(offset_dif,3))\n",
    "                    prev_offset = element.offset\n",
    "\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "                    offset_dif = float(element.offset-prev_offset)\n",
    "                    durations.append(str(element.quarterLength))\n",
    "                    offsets.append(round(offset_dif,3))\n",
    "                    prev_offset = element.offset\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total files: {total_files}\")\n",
    "    print(f\"Successfully processed: {count}\")\n",
    "    print(f\"Skipped/Invalid files: {skipped_files}\")\n",
    "    \n",
    "    if not notes:  # Check if we got any data\n",
    "        raise ValueError(\"No valid MIDI data was extracted from any files\")\n",
    "        \n",
    "    return [notes, offsets, durations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_data(directory, output_file=\"music_data.pkl\"):\n",
    "    # Save the processed data to a pkl file\n",
    "    try:\n",
    "        data = data_extractor(directory)\n",
    "        with open(output_file, 'wb') as f:  # 'wb' for write binary mode\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# To load the data later on\n",
    "def load_extracted_data(file_path=\"music_data.pkl\"):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load classical data from processed pkl file\n",
    "classical_data = load_extracted_data(\"pkl/classical_kaggle.pkl\")\n",
    "classical_note_data = classical_data[0]\n",
    "classical_offset_data = classical_data[1]\n",
    "classical_duration_data = classical_data[2]\n",
    "classical_unique_note_number = len(list(set(classical_note_data)))\n",
    "classical_unique_notes = sorted(list(set(classical_note_data)))\n",
    "classical_unique_offset_number = len(list(set(classical_offset_data)))\n",
    "classical_unique_offsets = sorted(list(set(classical_offset_data)))\n",
    "classical_unique_duration_number = len(list(set(classical_duration_data)))\n",
    "classical_unique_durations = sorted(list(set(classical_duration_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jazz data from processed pkl file\n",
    "jazz_data = load_extracted_data(\"pkl/jazz.pkl\")\n",
    "jazz_note_data = jazz_data[0]\n",
    "jazz_offset_data = jazz_data[1]\n",
    "jazz_duration_data = jazz_data[2]\n",
    "jazz_unique_note_number = len(list(set(jazz_note_data)))\n",
    "jazz_unique_notes = sorted(list(set(jazz_note_data)))\n",
    "jazz_unique_offset_number = len(list(set(jazz_offset_data)))\n",
    "jazz_unique_offsets = sorted(list(set(jazz_offset_data)))\n",
    "jazz_unique_duration_number = len(list(set(jazz_duration_data)))\n",
    "jazz_unique_durations = sorted(list(set(jazz_duration_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rock data from processed pkl file\n",
    "rock_data = load_extracted_data(\"pkl/rock.pkl\")\n",
    "rock_note_data = rock_data[0]\n",
    "rock_offset_data = rock_data[1]\n",
    "rock_duration_data = rock_data[2]\n",
    "rock_unique_note_number = len(list(set(rock_note_data)))\n",
    "rock_unique_notes = sorted(list(set(rock_note_data)))\n",
    "rock_unique_offset_number = len(list(set(rock_offset_data)))\n",
    "rock_unique_offsets = sorted(list(set(rock_offset_data)))\n",
    "rock_unique_duration_number = len(list(set(rock_duration_data)))\n",
    "rock_unique_durations = sorted(list(set(rock_duration_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, notes, offsets, durations, genre_labels, sequence_length=32):\n",
    "        # Store inputs and create mappings from musical elements to indices\n",
    "        self.notes = notes\n",
    "        self.offsets = offsets\n",
    "        self.durations = durations\n",
    "        self.genre_labels = genre_labels\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Create vocabularies\n",
    "        self.note_to_idx = {note: idx for idx, note in enumerate(set(notes))}\n",
    "        self.offset_to_idx = {offset: idx for idx, offset in enumerate(set(offsets))}\n",
    "        self.duration_to_idx = {duration: idx for idx, duration in enumerate(set(durations))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.notes) - self.sequence_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor([self.note_to_idx[note] for note in self.notes[idx:idx+self.sequence_length]]),\n",
    "            torch.tensor([self.offset_to_idx[offset] for offset in self.offsets[idx:idx+self.sequence_length]]),\n",
    "            torch.tensor([self.duration_to_idx[duration] for duration in self.durations[idx:idx+self.sequence_length]]),\n",
    "            torch.tensor(self.genre_labels[idx:idx+self.sequence_length])\n",
    "        )\n",
    "\n",
    "# The LSTM itself\n",
    "class GenreAwareMusicLSTM(nn.Module):\n",
    "    def __init__(self, note_vocab_size, offset_vocab_size, duration_vocab_size, \n",
    "                 num_genres, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Store vocab sizes as class attributes\n",
    "        self.note_vocab_size = note_vocab_size\n",
    "        self.offset_vocab_size = offset_vocab_size\n",
    "        self.duration_vocab_size = duration_vocab_size\n",
    "        \n",
    "        self.note_embedding = nn.Embedding(note_vocab_size, embedding_dim)\n",
    "        self.offset_embedding = nn.Embedding(offset_vocab_size, embedding_dim)\n",
    "        self.duration_embedding = nn.Embedding(duration_vocab_size, embedding_dim)\n",
    "        self.genre_embedding = nn.Embedding(num_genres, embedding_dim)\n",
    "        \n",
    "        # Combined embedding dimension\n",
    "        combined_dim = embedding_dim * 4  # notes + offsets + durations + genre\n",
    "        \n",
    "        # LSTM layer for sequence processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=combined_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Output layers for predicting next musical elements\n",
    "        self.note_fc = nn.Linear(hidden_dim, note_vocab_size)\n",
    "        self.offset_fc = nn.Linear(hidden_dim, offset_vocab_size)\n",
    "        self.duration_fc = nn.Linear(hidden_dim, duration_vocab_size)\n",
    "        \n",
    "    def forward(self, notes, offsets, durations, genre_labels):\n",
    "        # Embed all features\n",
    "        note_embeds = self.note_embedding(notes)\n",
    "        offset_embeds = self.offset_embedding(offsets)\n",
    "        duration_embeds = self.duration_embedding(durations)\n",
    "        genre_embeds = self.genre_embedding(genre_labels)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = torch.cat(\n",
    "            [note_embeds, offset_embeds, duration_embeds, genre_embeds],\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        \n",
    "        # Generate predictions\n",
    "        note_logits = self.note_fc(lstm_out)\n",
    "        offset_logits = self.offset_fc(lstm_out)\n",
    "        duration_logits = self.duration_fc(lstm_out)\n",
    "        \n",
    "        return note_logits, offset_logits, duration_logits\n",
    "    \n",
    "    def generate(self, genre_id, seed_sequence, max_length=250, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate new music sequence based on genre and initial seed\n",
    "        - Higher temperature = more random/creative\n",
    "        - Lower temperature = more conservative/predictable\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            current_sequence = seed_sequence\n",
    "            genre_labels = torch.full_like(current_sequence[0], genre_id)\n",
    "            \n",
    "            generated_notes = []\n",
    "            generated_offsets = []\n",
    "            generated_durations = []\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # Get predictions for next musical elements\n",
    "                note_logits, offset_logits, duration_logits = self(\n",
    "                    current_sequence[0],\n",
    "                    current_sequence[1],\n",
    "                    current_sequence[2],\n",
    "                    genre_labels\n",
    "                )\n",
    "                \n",
    "                # Apply temperature\n",
    "                note_logits = note_logits[:, -1, :] / temperature\n",
    "                offset_logits = offset_logits[:, -1, :] / temperature\n",
    "                duration_logits = duration_logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Sample from distributions\n",
    "                note_probs = F.softmax(note_logits, dim=-1)\n",
    "                offset_probs = F.softmax(offset_logits, dim=-1)\n",
    "                duration_probs = F.softmax(duration_logits, dim=-1)\n",
    "                \n",
    "                next_note = torch.multinomial(note_probs, 1)\n",
    "                next_offset = torch.multinomial(offset_probs, 1)\n",
    "                next_duration = torch.multinomial(duration_probs, 1)\n",
    "                \n",
    "                # store generated elements\n",
    "                generated_notes.append(next_note.item())\n",
    "                generated_offsets.append(next_offset.item())\n",
    "                generated_durations.append(next_duration.item())\n",
    "                \n",
    "                # Update current sequence\n",
    "                current_sequence = (\n",
    "                    torch.cat([current_sequence[0][:, 1:], next_note], dim=1),\n",
    "                    torch.cat([current_sequence[1][:, 1:], next_offset], dim=1),\n",
    "                    torch.cat([current_sequence[2][:, 1:], next_duration], dim=1)\n",
    "                )\n",
    "                \n",
    "            return generated_notes, generated_offsets, generated_durations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fraction_to_float(fraction_str):\n",
    "    \"\"\"Convert string fractions to float values\"\"\"\n",
    "    if '/' in fraction_str:\n",
    "        num, denom = fraction_str.split('/')\n",
    "        return float(num) / float(denom)\n",
    "    return float(fraction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model, optimizer, epoch, genre, path):\n",
    "    # save the model at its current point in training in case it crashes\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'genre': genre\n",
    "    }, path)\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, path):\n",
    "    # load the saved model chekpoint from file\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, batch_size=48, num_epochs=25, sequence_length=32, \n",
    "                learning_rate=0.001, genre_id=0, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 3\n",
    "    min_loss = float('inf')\n",
    "    no_improve_count = 0\n",
    "    \n",
    "    notes, offsets, durations = data\n",
    "    genre_labels = [genre_id] * len(notes)\n",
    "    \n",
    "    dataset = MusicDataset(\n",
    "        notes=notes,\n",
    "        offsets=offsets,\n",
    "        durations=durations,\n",
    "        genre_labels=genre_labels,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            note_seq = batch[0].to(device)\n",
    "            offset_seq = batch[1].to(device)\n",
    "            duration_seq = batch[2].to(device)\n",
    "            genre_seq = batch[3].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            note_logits, offset_logits, duration_logits = model(\n",
    "                note_seq, offset_seq, duration_seq, genre_seq\n",
    "            )\n",
    "            \n",
    "            batch_size, seq_len = note_seq.shape\n",
    "            \n",
    "            target_notes = note_seq[:, 1:].reshape(-1)\n",
    "            target_offsets = offset_seq[:, 1:].reshape(-1)\n",
    "            target_durations = duration_seq[:, 1:].reshape(-1)\n",
    "            \n",
    "            note_logits = note_logits[:, :-1, :].reshape(-1, model.note_vocab_size)\n",
    "            offset_logits = offset_logits[:, :-1, :].reshape(-1, model.offset_vocab_size)\n",
    "            duration_logits = duration_logits[:, :-1, :].reshape(-1, model.duration_vocab_size)\n",
    "            \n",
    "            note_loss = criterion(note_logits, target_notes)\n",
    "            offset_loss = criterion(offset_logits, target_offsets)\n",
    "            duration_loss = criterion(duration_logits, target_durations)\n",
    "            \n",
    "            loss = note_loss + offset_loss + duration_loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (len(progress_bar))})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            no_improve_count = 0\n",
    "            # Save best model\n",
    "            save_model_checkpoint(model, optimizer, epoch, f\"genre_{genre_id}\", \n",
    "                                f\"best_model.pt\")\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            print(f\"Loss did not improve for {no_improve_count} epochs\")\n",
    "        \n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_model_checkpoint(model, optimizer, epoch, f\"genre_{genre_id}\", \n",
    "                                f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GenreAwareMusicLSTM(\n",
    "    note_vocab_size=classical_unique_note_number,\n",
    "    offset_vocab_size=classical_unique_offset_number,\n",
    "    duration_vocab_size=classical_unique_duration_number,\n",
    "    num_genres=3,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256\n",
    ")\n",
    "\n",
    "# Train on classical music (genre_id = 0)\n",
    "classical_data = (classical_note_data, classical_offset_data, classical_duration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added this because the different genres have different amounts of weights/unique notes\n",
    "def expand_embedding_layer(old_model, new_vocab_size, embedding_name):\n",
    "    \"\"\"\n",
    "    Expand an embedding layer while preserving existing weights\n",
    "    \"\"\"\n",
    "    old_embedding = getattr(old_model, embedding_name)\n",
    "    old_vocab_size, embedding_dim = old_embedding.weight.shape\n",
    "    \n",
    "    # Create new embedding layer with larger vocabulary\n",
    "    new_embedding = nn.Embedding(new_vocab_size, embedding_dim)\n",
    "    \n",
    "    # Copy old weights\n",
    "    with torch.no_grad():\n",
    "        new_embedding.weight[:old_vocab_size] = old_embedding.weight\n",
    "        \n",
    "        # Initialize new embeddings with mean and std of existing embeddings\n",
    "        if new_vocab_size > old_vocab_size:\n",
    "            mean = old_embedding.weight.mean().item()\n",
    "            std = old_embedding.weight.std().item()\n",
    "            nn.init.normal_(new_embedding.weight[old_vocab_size:], mean=mean, std=std)\n",
    "    \n",
    "    return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added this because the different genres have different amounts of weights/unique notes\n",
    "def expand_model_vocabulary(model, new_note_size, new_offset_size, new_duration_size):\n",
    "    \"\"\"\n",
    "    Expand model vocabulary while preserving trained weights\n",
    "    \"\"\"\n",
    "    # Save old parameters\n",
    "    old_params = {\n",
    "        'embedding_dim': model.note_embedding.embedding_dim,\n",
    "        'hidden_dim': model.lstm.hidden_size,\n",
    "        'num_genres': model.genre_embedding.num_embeddings\n",
    "    }\n",
    "    \n",
    "    # Create new embeddings\n",
    "    new_note_embedding = expand_embedding_layer(model, new_note_size, 'note_embedding')\n",
    "    new_offset_embedding = expand_embedding_layer(model, new_offset_size, 'offset_embedding')\n",
    "    new_duration_embedding = expand_embedding_layer(model, new_duration_size, 'duration_embedding')\n",
    "    \n",
    "    # Create new model\n",
    "    new_model = GenreAwareMusicLSTM(\n",
    "        note_vocab_size=new_note_size,\n",
    "        offset_vocab_size=new_offset_size,\n",
    "        duration_vocab_size=new_duration_size,\n",
    "        num_genres=old_params['num_genres'],\n",
    "        embedding_dim=old_params['embedding_dim'],\n",
    "        hidden_dim=old_params['hidden_dim']\n",
    "    )\n",
    "    \n",
    "    # Copy expanded embeddings\n",
    "    new_model.note_embedding = new_note_embedding\n",
    "    new_model.offset_embedding = new_offset_embedding\n",
    "    new_model.duration_embedding = new_duration_embedding\n",
    "    \n",
    "    # Copy genre embedding and LSTM weights (these don't change size)\n",
    "    new_model.genre_embedding.load_state_dict(model.genre_embedding.state_dict())\n",
    "    new_model.lstm.load_state_dict(model.lstm.state_dict())\n",
    "    \n",
    "    # Copy FC layer weights\n",
    "    with torch.no_grad():\n",
    "        # Note FC layer\n",
    "        old_note_size = model.note_fc.weight.shape[0]\n",
    "        new_model.note_fc.weight[:old_note_size] = model.note_fc.weight\n",
    "        new_model.note_fc.bias[:old_note_size] = model.note_fc.bias\n",
    "        \n",
    "        # Offset FC layer\n",
    "        old_offset_size = model.offset_fc.weight.shape[0]\n",
    "        new_model.offset_fc.weight[:old_offset_size] = model.offset_fc.weight\n",
    "        new_model.offset_fc.bias[:old_offset_size] = model.offset_fc.bias\n",
    "        \n",
    "        # Duration FC layer\n",
    "        old_duration_size = model.duration_fc.weight.shape[0]\n",
    "        new_model.duration_fc.weight[:old_duration_size] = model.duration_fc.weight\n",
    "        new_model.duration_fc.bias[:old_duration_size] = model.duration_fc.bias\n",
    "        \n",
    "        # Initialize new FC weights with statistics of old weights\n",
    "        def init_remaining_weights(layer, old_size):\n",
    "            if layer.weight.shape[0] > old_size:\n",
    "                mean = layer.weight[:old_size].mean().item()\n",
    "                std = layer.weight[:old_size].std().item()\n",
    "                nn.init.normal_(layer.weight[old_size:], mean=mean, std=std)\n",
    "                nn.init.normal_(layer.bias[old_size:], mean=mean, std=std)\n",
    "        \n",
    "        init_remaining_weights(new_model.note_fc, old_note_size)\n",
    "        init_remaining_weights(new_model.offset_fc, old_offset_size)\n",
    "        init_remaining_weights(new_model.duration_fc, old_duration_size)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train the model on classical music for the first time\n",
    "model, optimizer = train_model(\n",
    "    model=model,\n",
    "    data=classical_data,\n",
    "    batch_size=48,\n",
    "    num_epochs=25,\n",
    "    sequence_length=32,\n",
    "    genre_id=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get combined vocabulary sizes because numbers are different between the datasets\n",
    "# Change values below depending on order of training\n",
    "combined_note_number = len(set(list(classical_note_data) + list(rock_note_data)))\n",
    "combined_offset_number = len(set(list(classical_offset_data) + list(rock_offset_data)))\n",
    "combined_duration_number = len(set(list(classical_duration_data) + list(rock_duration_data)))\n",
    "\n",
    "# Initialize original model\n",
    "model = GenreAwareMusicLSTM(\n",
    "    note_vocab_size=classical_unique_note_number,\n",
    "    offset_vocab_size=classical_unique_offset_number,\n",
    "    duration_vocab_size=classical_unique_duration_number,\n",
    "    num_genres=3,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256\n",
    ")\n",
    "\n",
    "# Initialize optimizer and load checkpoint\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch, genre = load_model_checkpoint(model, optimizer, \"best_model.pt\")\n",
    "\n",
    "# Expand model vocabulary\n",
    "model = expand_model_vocabulary(\n",
    "    model,\n",
    "    new_note_size=combined_note_number,\n",
    "    new_offset_size=combined_offset_number,\n",
    "    new_duration_size=combined_duration_number\n",
    ")\n",
    "\n",
    "# Create new optimizer for expanded model (after training for classical)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Now train on jazz data\n",
    "model, optimizer = train_model(\n",
    "    model=model,\n",
    "    data=jazz_data,\n",
    "    genre_id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_note_number = len(set(list(classical_note_data) + list(jazz_note_data)+ list(rock_duration_data)))\n",
    "combined_offset_number = len(set(list(classical_offset_data) + list(jazz_offset_data) + list(rock_duration_data)))\n",
    "combined_duration_number = len(set(list(classical_duration_data) + list(jazz_duration_data) + list(rock_duration_data)))\n",
    "\n",
    "# train on rock_data\n",
    "model, optimizer = train_model(\n",
    "    model=model,\n",
    "    data=rock_data,\n",
    "    genre_id=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to Actual Music Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi_from_prediction(generated_notes, generated_offsets, generated_durations, \n",
    "                              idx_to_note, idx_to_offset, idx_to_duration, \n",
    "                              output_file=\"generated_music.mid\"): \n",
    "    \"\"\"\n",
    "    Convert generated sequences back to a MIDI file with limited chord size\n",
    "    \"\"\"\n",
    "    output_notes = stream.Stream()\n",
    "\n",
    "    current_offset = 0.0\n",
    "    \n",
    "    def number_to_pitch(number):\n",
    "        try:\n",
    "            if isinstance(number, str) and number.isdigit():\n",
    "                number = int(number)\n",
    "            \n",
    "            # limiting the octaves the music can be generated in (for some reason it prefers super low notes)\n",
    "            MIN_MIDI = 48  # C3\n",
    "            MAX_MIDI = 83  # B6\n",
    "            \n",
    "            while number < MIN_MIDI:\n",
    "                number += 12\n",
    "            while number > MAX_MIDI:\n",
    "                number -= 12\n",
    "                \n",
    "            p = pitch.Pitch()\n",
    "            p.midi = number\n",
    "            return p.nameWithOctave\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    for i in range(len(generated_notes)):\n",
    "        try:\n",
    "            note_str = str(idx_to_note[generated_notes[i]])\n",
    "            offset_val = convert_fraction_to_float(str(idx_to_offset[generated_offsets[i]]))\n",
    "            duration_val = convert_fraction_to_float(str(idx_to_duration[generated_durations[i]]))\n",
    "            \n",
    "            current_offset += offset_val\n",
    "            \n",
    "            if '.' in note_str:  # It's a chord\n",
    "                notes_in_chord = note_str.split('.')\n",
    "                chord_notes = []\n",
    "                # Only take up to max_chord_size notes for the chord, setting it to 1 produces single-note lines\n",
    "                for current_note in notes_in_chord[:2]: # max chord size, set to 2 so that there aren't 5-note chords\n",
    "                    try:\n",
    "                        if current_note.isdigit():\n",
    "                            pitch_name = number_to_pitch(int(current_note))\n",
    "                            if pitch_name:\n",
    "                                new_note = note.Note(pitch_name)\n",
    "                                chord_notes.append(new_note)\n",
    "                    except:\n",
    "                        continue\n",
    "                if chord_notes:\n",
    "                    new_chord = chord.Chord(chord_notes)\n",
    "                    new_chord.offset = current_offset\n",
    "                    new_chord.quarterLength = duration_val\n",
    "                    output_notes.append(new_chord)\n",
    "            else:  # It's a note\n",
    "                if note_str.isdigit():\n",
    "                    pitch_name = number_to_pitch(int(note_str))\n",
    "                    if pitch_name:\n",
    "                        new_note = note.Note(pitch_name)\n",
    "                        new_note.offset = current_offset\n",
    "                        new_note.quarterLength = duration_val\n",
    "                        output_notes.append(new_note)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping problematic note/chord: {note_str}\")\n",
    "            continue\n",
    "    \n",
    "    if len(output_notes) == 0:\n",
    "        raise ValueError(\"No valid notes were generated\")\n",
    "        \n",
    "    output_notes.write('midi', fp=output_file)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, genre_id, sequence_length, idx_to_note, idx_to_offset, idx_to_duration,\n",
    "                  seed_sequence=None, max_length=250, temperature=1.0,\n",
    "                  output_file=\"generated_music.mid\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Generate music using the trained model and save it as a MIDI file\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained GenreAwareMusicLSTM model\n",
    "    genre_id: int, which genre to generate (0=classical, 1=jazz, etc.)\n",
    "    sequence_length: int, length of input sequences used during training\n",
    "    idx_to_note/offset/duration: dictionaries to convert indices back to musical values\n",
    "    seed_sequence: optional tuple of (notes, offsets, durations) to start generation\n",
    "    max_length: int, number of notes to generate\n",
    "    output_file: string, where to save the MIDI file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # If no seed sequence provided, create a random one\n",
    "    if seed_sequence is None:\n",
    "        # Create a random seed sequence\n",
    "        seed_notes = np.random.randint(0, len(idx_to_note), sequence_length)\n",
    "        seed_offsets = np.random.randint(0, len(idx_to_offset), sequence_length)\n",
    "        seed_durations = np.random.randint(0, len(idx_to_duration), sequence_length)\n",
    "        \n",
    "        seed_sequence = (\n",
    "            torch.tensor([seed_notes]).to(device),\n",
    "            torch.tensor([seed_offsets]).to(device),\n",
    "            torch.tensor([seed_durations]).to(device)\n",
    "        )\n",
    "    \n",
    "    # Generate the music\n",
    "    with torch.no_grad():\n",
    "        generated_notes, generated_offsets, generated_durations = model.generate(\n",
    "            genre_id=genre_id,\n",
    "            seed_sequence=seed_sequence,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    # Convert to MIDI and save\n",
    "    midi_file = create_midi_from_prediction(\n",
    "        generated_notes, \n",
    "        generated_offsets, \n",
    "        generated_durations,\n",
    "        idx_to_note,\n",
    "        idx_to_offset,\n",
    "        idx_to_duration,\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    return midi_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "# may not run properly depending on size of model & vocab size which you might need to change\n",
    "model = GenreAwareMusicLSTM(\n",
    "    note_vocab_size=682, # matching the saved model's size\n",
    "    offset_vocab_size=330,\n",
    "    duration_vocab_size=103, \n",
    "    num_genres=3,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256\n",
    ")\n",
    "\n",
    "# Initialize optimizer (needed for loading the checkpoint)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Load the saved model from pt file (trained in other notebook)\n",
    "epoch, genre = load_model_checkpoint(model, optimizer, \"best_model.pt\")\n",
    "\n",
    "# Create dataset to get the mappings\n",
    "dataset = MusicDataset(\n",
    "    notes=classical_note_data,\n",
    "    offsets=classical_offset_data,\n",
    "    durations=classical_duration_data,\n",
    "    genre_labels=[0] * len(classical_note_data),\n",
    "    sequence_length=32\n",
    ")\n",
    "\n",
    "# Get the inverse mappings\n",
    "idx_to_note = {idx: note for note, idx in dataset.note_to_idx.items()}\n",
    "idx_to_offset = {idx: offset for offset, idx in dataset.offset_to_idx.items()}\n",
    "idx_to_duration = {idx: duration for duration, idx in dataset.duration_to_idx.items()}\n",
    "\n",
    "# Generate music with loaded model\n",
    "generated_file = generate_music(\n",
    "    model=model,\n",
    "    genre_id=0, # Classical 0, Jazz 1, Rock 2\n",
    "    sequence_length=32,\n",
    "    idx_to_note=idx_to_note,\n",
    "    idx_to_offset=idx_to_offset,\n",
    "    idx_to_duration=idx_to_duration,\n",
    "    temperature=1.5,\n",
    "    output_file=\"my_piece.mid\"\n",
    ")\n",
    "\n",
    "# output should be saved as output_file above\n",
    "# MIDIs can be viewed and played with applications such as Musescore or Finale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
